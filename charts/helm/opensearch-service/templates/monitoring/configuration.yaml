{{- if (eq (include "monitoring.enabled" .) "true") }}
apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ template "opensearch.fullname" . }}-monitoring-configuration
  labels:
{{ include "opensearch.labels.standard" . | indent 4 }}
{{ include "opensearch-service.defaultLabels" . | indent 4 }}
    name: {{ template "opensearch.fullname" . }}-monitoring
    component: opensearch-monitoring
data:
  config: |-
    # Telegraf Configuration
    #
    # Telegraf is entirely plugin driven. All metrics are gathered from the
    # declared inputs, and sent to the declared outputs.
    #
    # Plugins must be declared in here to be active.
    # To deactivate a plugin, comment out the name and any variables.
    #
    # Use 'telegraf -config telegraf.conf -test' to see what metrics a config
    # file would generate.
    #
    # Environment variables can be used anywhere in this config file, simply prepend
    # them with $. For strings the variable must be within quotes (ie, "$STR_VAR"),
    # for numbers and booleans they should be plain (ie, $INT_VAR, $BOOL_VAR)


    # Global tags can be specified here in key="value" format.
    [global_tags]
      # dc = "us-east-1" # will tag all metrics with dc=us-east-1
      # rack = "1a"
      ## Environment variables can be used as tags, and throughout the config file
      # user = "$USER"
      project_name= "$OS_PROJECT"


    # Configuration for telegraf agent
    [agent]
      ## Default data collection interval for all inputs
      interval = "$ELASTICSEARCH_EXEC_PLUGIN_TIMEOUT"
      ## Rounds collection interval to 'interval'
      ## ie, if interval="10s" then always collect on :00, :10, :20, etc.
      round_interval = true

      ## Telegraf will send metrics to outputs in batches of at most
      ## metric_batch_size metrics.
      ## This controls the size of writes that Telegraf sends to output plugins.
      metric_batch_size = 1000

      ## For failed writes, telegraf will cache metric_buffer_limit metrics for each
      ## output, and will flush this buffer on a successful write. Oldest metrics
      ## are dropped first when this buffer fills.
      ## This buffer only fills when writes fail to output plugin(s).
      metric_buffer_limit = 10000

      ## Collection jitter is used to jitter the collection by a random amount.
      ## Each plugin will sleep for a random time within jitter before collecting.
      ## This can be used to avoid many plugins querying things like sysfs at the
      ## same time, which can have a measurable effect on the system.
      collection_jitter = "0s"

      ## Default flushing interval for all outputs. You shouldn't set this below
      ## interval. Maximum flush_interval will be flush_interval + flush_jitter
      flush_interval = "10s"
      ## Jitter the flush interval by a random amount. This is primarily to avoid
      ## large write spikes for users running a large number of telegraf instances.
      ## ie, a jitter of 5s and interval 10s means flushes will happen every 10-15s
      flush_jitter = "0s"

      ## By default, precision will be set to the same timestamp order as the
      ## collection interval, with the maximum being 1s.
      ## Precision will NOT be used for service inputs, such as logparser and statsd.
      ## Valid values are "ns", "us" (or "Âµs"), "ms", "s".
      precision = ""

      ## Logging configuration:
      ## Run telegraf with debug log messages.
      debug = true
      ## Run telegraf in quiet mode (error log messages only).
      quiet = false
      ## Specify the log file name. The empty string means to log to stderr.
      logfile = ""

      ## Override default hostname, if empty use os.Hostname()
      hostname = ""
      ## If set to true, do no set the "host" tag in the telegraf agent.
      omit_hostname = false


    ###############################################################################
    #                            OUTPUT PLUGINS                                   #
    ###############################################################################

    {{- if (eq .Values.monitoring.monitoringType "influxdb") }}
    # Configuration for influxdb server to send metrics to
    [[outputs.influxdb]]
      ## The full HTTP or UDP endpoint URL for your InfluxDB instance.
      ## Multiple urls can be specified as part of the same cluster,
      ## this means that only ONE of the urls will be written to each interval.
      # urls = ["udp://localhost:8089"] # UDP endpoint example
      urls = ["$SM_DB_HOST"] # required
      ## The target database for metrics (telegraf will create it if not exists).
      database = "$SM_DB_NAME" # required

      ## Retention policy to write to. Empty string writes to the default rp.
      retention_policy = ""
      ## Write consistency (clusters only), can be: "any", "one", "quorum", "all"
      write_consistency = "any"

      ## Write timeout (for the InfluxDB client), formatted as a string.
      ## If not provided, will default to 5s. 0s means no timeout (not recommended).
      timeout = "5s"
      username = "$SM_DB_USERNAME"
      password = "$SM_DB_PASSWORD"
      ## Set the user agent for HTTP POSTs (can be useful for log differentiation)
      # user_agent = "telegraf"
      ## Set UDP payload size, defaults to InfluxDB UDP Client default (512 bytes)
      # udp_payload = 512

      ## Optional SSL Config
      # tls_ca = "/etc/telegraf/ca.pem"
      # ssl_cert = "/etc/telegraf/cert.pem"
      # ssl_key = "/etc/telegraf/key.pem"
      ## Use SSL but skip chain & host verification
      # insecure_skip_verify = false
    {{- else }}
    # Publish all metrics to /metrics for Prometheus to scrape
    [[outputs.prometheus_client]]
      ## Address to listen on.
      listen = ":8096"

      ## Metric version controls the mapping from Telegraf metrics into
      ## Prometheus format.  When using the prometheus input, use the same value in
      ## both plugins to ensure metrics are round-tripped without modification.
      ##
      ##   example: metric_version = 1; deprecated in 1.13
      ##            metric_version = 2; recommended version
      # metric_version = 1

      ## Use HTTP Basic Authentication.
      # basic_username = "Foo"
      # basic_password = "Bar"

      ## If set, the IP Ranges which are allowed to access metrics.
      ##   ex: ip_range = ["192.168.0.0/24", "192.168.1.0/30"]
      # ip_range = []

      ## Path to publish the metrics on.
      # path = "/metrics"

      ## Expiration interval for each metric. 0 == no expiration
      # expiration_interval = "60s"
      {{- if and (not .Values.global.externalOpensearch.enabled) .Values.monitoring.slowQueries.enabled }}
      expiration_interval = "${PROCESSING_INTERVAL_MINUTES}m"
      {{- end }}

      ## Collectors to enable, valid entries are "gocollector" and "process".
      ## If unset, both are enabled.
      # collectors_exclude = ["gocollector", "process"]

      ## Send string metrics as Prometheus labels.
      ## Unless set to false all string metrics will be sent as labels.
      # string_as_label = true

      ## If set, enable TLS with the given certificate.
      # tls_cert = "/etc/ssl/telegraf.crt"
      # tls_key = "/etc/ssl/telegraf.key"

      ## Set one or more allowed client CA certificate file names to
      ## enable mutually authenticated TLS connections
      # tls_allowed_cacerts = ["/etc/telegraf/clientca.pem"]

      ## Export metric collection time.
      # export_timestamp = false
    {{- end }}

    ###############################################################################
    #                            PROCESSOR PLUGINS                                #
    ###############################################################################

    [[processors.strings]]
      # Replace all non-overlapping instances of old with new
      [[processors.strings.replace]]
        measurement = "*"
        old = "elasticsearch"
        new = "opensearch"

    ###############################################################################
    #                            INPUT PLUGINS                                    #
    ###############################################################################

    # Read metrics about cpu usage
    [[inputs.cpu]]
      ## Whether to report per-cpu stats or not
      percpu = true
      ## Whether to report total system cpu stats or not
      totalcpu = true
      ## If true, collect raw CPU time metrics.
      collect_cpu_time = false
      ##Specifies a prefix to attach to the measurement name.


    # Read metrics about disk usage by mount point
    [[inputs.disk]]
      ## By default, telegraf gather stats for all mountpoints.
      ## Setting mountpoints will restrict the stats to the specified mountpoints.
      # mount_points = ["/"]

      ## Ignore some mountpoints by filesystem type. For example (dev)tmpfs (usually
      ## present on /run, /var/run, /dev/shm or /dev).
      ignore_fs = ["tmpfs", "devtmpfs"]
      ##Specifies a prefix to attach to the measurement name.


    # Read metrics about disk IO by device
    [[inputs.diskio]]
      ## By default, telegraf will gather stats for all devices including
      ## disk partitions.
      ## Setting devices will restrict the stats to the specified devices.
      # devices = ["sda", "sdb"]
      ## Uncomment the following line if you need disk serial numbers.
      # skip_serial_number = false
      ##Specifies a prefix to attach to the measurement name.


    # Get kernel statistics from /proc/stat
    [[inputs.kernel]]
      # no configuration
      ##Specifies a prefix to attach to the measurement name.

    # Read metrics about memory usage
    [[inputs.mem]]
      # no configuration
      ##Specifies a prefix to attach to the measurement name.

    # Get the number of processes and group them by status
    [[inputs.processes]]
      # no configuration
      ##Specifies a prefix to attach to the measurement name.

    # Read metrics about swap memory usage
    [[inputs.swap]]
      # no configuration
      ##Specifies a prefix to attach to the measurement name.

    # Read metrics about system load & uptime
    [[inputs.system]]
      # no configuration
      ##Specifies a prefix to attach to the measurement name.

    # Read stats from one or more OpenSearch servers or clusters
    [[inputs.elasticsearch]]
      # An array of glob pattern strings.
      # Fields with a field key matching one of the patterns will be discarded from the metric.
      fieldexclude = ["status_code", "plugins_*", "__segments_segment_replication_*"]

      ## specify a list of one or more OpenSearch servers
      # you can add username and password to your url to use basic authentication:
      # servers = ["http://user:pass@localhost:9200"]
      servers = ["$ELASTICSEARCH_PROTOCOL://$ELASTICSEARCH_HOST:$ELASTICSEARCH_PORT"]

      ## Timeout for HTTP requests to the OpenSearch server(s)
      timeout = "8s"

      ## When local is true (the default), the node will read only its own stats.
      ## Set local to false when you want to read the node stats from all nodes
      ## of the cluster.
      local = false

      ## Set cluster_health to true when you want to also obtain cluster health stats
      cluster_health = true

      ## Set cluster_stats to true when you want to also obtain cluster stats from the
      ## Master node.
      cluster_stats = true
    {{ if .Values.monitoring.includeIndices }}
      ## Indices to collect; can be one or more indices names or _all
      ## Use of wildcards is allowed. Use a wildcard at the end to retrieve index names that end with a changing value, like a date.
      indices_include = ["_all"]
    {{- end }}

      ## HTTP Basic Authentication username and password.
      username = "$ELASTICSEARCH_USERNAME"
      password = "$ELASTICSEARCH_PASSWORD"

      ## Optional SSL Config
      # tls_ca = "/etc/telegraf/ca.pem"
      # ssl_cert = "/etc/telegraf/cert.pem"
      # ssl_key = "/etc/telegraf/key.pem"
      ## Use SSL but skip chain & host verification
    {{- if eq (include "opensearch.tlsEnabled" .) "true" }}
      insecure_skip_verify = false
      tls_ca = "/trusted-certs/root-ca.pem"
    {{- end }}

      ##Specifies a prefix to attach to the measurement name.


    # Read metrics from one or more commands that can output to stdout
    [[inputs.exec]]
      ## Commands array
      commands = [
        {{- if (eq (include "opensearch.enableDisasterRecovery" .) "true") }}
        "python3 /opt/elasticsearch-monitoring/exec-scripts/replication_metric.py",
        {{- end }}
        "python3 /opt/elasticsearch-monitoring/exec-scripts/health_metric.py",
        "python3 /opt/elasticsearch-monitoring/exec-scripts/backup_metric.py",
        "python3 /opt/elasticsearch-monitoring/exec-scripts/dbaas_health_metric.py"
      ]

      ## Timeout for each command to complete.
      timeout = "$ELASTICSEARCH_EXEC_PLUGIN_TIMEOUT"

      ## measurement name suffix (for separating different commands)
      # name_prefix = "elasticsearch_"

      ## Data format to consume.
      ## Each data format has it's own unique set of configuration options, read
      ## more about them here:
      ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md
      data_format = "influx"

    {{- if and (not .Values.global.externalOpensearch.enabled) .Values.monitoring.slowQueries.enabled }}
    # Read metrics from one or more commands that can output to stdout
    [[inputs.exec]]
      ## Commands array
      commands = [
        "python3 /opt/elasticsearch-monitoring/exec-scripts/slow_queries_metric.py"
      ]

      interval = "${PROCESSING_INTERVAL_MINUTES}m"

      ## Timeout for each command to complete.
      timeout = "${PROCESSING_INTERVAL_MINUTES}m"

      ## measurement name suffix (for separating different commands)
      # name_prefix = "elasticsearch_"

      ## Data format to consume.
      ## Each data format has it's own unique set of configuration options, read
      ## more about them here:
      ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md
      data_format = "influx"
    {{- end }}
{{- end }}
